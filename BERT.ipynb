{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "## Homework 3, CS678 Spring 2024\n",
        "\n",
        "### This homework has two submission deadlines: (1) Checkpoint 1 on March 18, 2024, and (2) Checkpoint 2 on March 29, 2024.\n",
        "\n",
        "Submit the report to Gradescope with the naming convention of\n",
        "**John_Doe_HW3_Report_CS678_S24_ckpt1/2.pdf** if your name is John Doe and you are submitting for checkpoint 1/2.\n",
        "Only the report should be submitted to Gradescope. The rest of the code and data files,\n",
        "including this notebook in its completion form,\n",
        "should be submitted to Blackboard in a single zipped folder.\n",
        "\n",
        "<!-- #### IMPORTANT:\n",
        "\n",
        "After copying this notebook to your Google Drive, please paste a link to it\n",
        "below. To get a publicly-accessible link, hit the *Share* button at the top\n",
        "right, then click \"Get shareable link\" and copy over the result.If you fail to\n",
        "do this, you will receive no credit for this homework!\n",
        "\n",
        "***LINK:***\n",
        "\n",
        "--- -->\n",
        "\n",
        "\n",
        "##### *How to submit this problem set:*\n",
        "- Write all the answers in this notebook.\n",
        "\n",
        "- When creating your final version of the notebook to hand in,\n",
        "  please do a fresh restart and execute every cell in order.\n",
        "  One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
        "\n",
        "##### *Policy regarding Google Colab:*\n",
        "- The instruction in this notebook assumes that you will use Colab.\n",
        "\n",
        "- However, using Colab is not required. You are free to run the code on your local machine, though in that case you may suffer from a slow runtime due to the lack of proper GPU resources.\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the notebooks from a set number of students, chosen at\n",
        "  random. The audits will check that the code you wrote actually generates the\n",
        "  answers in your report PDF. If you turn in correct answers on your PDF without code\n",
        "  that actually generates those answers, we will consider this a serious case of\n",
        "  cheating. See the course page for honesty policies.\n",
        "\n",
        "- We will also run automatic checks of notebooks for plagiarism.\n",
        "  Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dYC4HbZL-j"
      },
      "source": [
        "# Checkpoint 1: Data Collection and Annotation\n",
        "\n",
        "In this homework, you will first collect a labeled dataset of **150** sentences for a text classification task of your choice. This process will include:\n",
        "\n",
        "1. *Data collection*: Collect 150 sentences from any source you find interesting (e.g., literature, Tweets, news articles, reviews, etc.)\n",
        "\n",
        "2. *Task design*: Come up with a multilabel sentence-level classification task that you would like to perform on your sentences.\n",
        "\n",
        "3. On your dataset, collect annotations from **two** classmates for your task on a **second, separate set** of a minimum of **150** sentences. Everyone in this class will need to both create their own dataset and also serve as an annotator for two other classmates. In order to get everything done on time, you need to complete the following steps:\n",
        "\n",
        "> *   Find two classmates willing to label 150 sentences each (use the Piazza \"search for teammates\" thread if you're having issues finding labelers).\n",
        "*   Collect the labeled data from each of the two annotators.\n",
        "*   Sanity check the data for basic cleanliness (are all examples annotated? are all labels allowable ones?)\n",
        "\n",
        "4. Collect feedback from annotators about the task including annotation time and obstacles encountered (e.g., maybe some sentences were particularly hard to annotate!)\n",
        "\n",
        "5. Calculate and report inter-annotator agreement.\n",
        "\n",
        "6. Aggregate output from both annotators to create final dataset (include your first 150 sentences too).\n",
        "\n",
        "7. Perform NLP experiments on your new dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHfrhY8lxP4"
      },
      "source": [
        "The mapping of label names and IDs in seed.tsv is as follows:\n",
        "\n",
        "```json\n",
        "{\n",
        "    'Economic': 1.0,\n",
        "    'Capacity and Resources': 2.0,\n",
        "    'Morality': 3.0,\n",
        "    'Fairness and Equality': 4.0,\n",
        "    'Legality, Constitutionality, Jurisdiction': 5.0,\n",
        "    'Policy Prescription and Evaluation': 6.0,\n",
        "    'Crime and Punishment': 7.0,\n",
        "    'Security and Defense': 8.0,\n",
        "    'Health and Safety': 9.0,\n",
        "    'Quality of Life': 10.0,\n",
        "    'Cultural Identity': 11.0,\n",
        "    'Public Sentiment': 12.0,\n",
        "    'Political': 13.0,\n",
        "    'External Regulation and Reputation': 14.0,\n",
        "    'Other': 15.0\n",
        "}\n",
        "```\n",
        "\n",
        "Make sure that this mapping is followed in all of your data files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltte0Z-vD0u"
      },
      "source": [
        "## Question 3 (8 points):\n",
        "Now, compute the inter-annotator agreement between your two annotators. Upload both .tsv files to your Colab session (click the folder icon in the sidebar to the left of the screen). In the code cell below, read the data from the two files and compute both the raw agreement (% of examples for which both annotators agreed on the label) and the [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa). Feel free to use implementations in existing libraries (e.g., [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)). After you’re done, report the raw agreement and Cohen’s scores in your report.\n",
        "\n",
        "*If you're curious, Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD7yvkwgy82S"
      },
      "outputs": [],
      "source": [
        "### WRITE CODE TO LOAD ANNOTATIONS AND\n",
        "### COMPUTE AGREEMENT + COHEN'S KAPPA HERE!\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def raw_agreement(annotator1_df, annotator2_df):\n",
        "    # TODO: Implement this function\n",
        "    df1 = pd.read_csv(annotator1_df, sep='\\t')\n",
        "    df2 = pd.read_csv(annotator2_df, sep='\\t')\n",
        "    label1 =  df1['label']\n",
        "    label2 =  df2['label']\n",
        "\n",
        "    count_same = 0\n",
        "    for i in range(len(label1)):\n",
        "        if(label1[i] == label2[i]):\n",
        "            count_same = count_same+1\n",
        "\n",
        "\n",
        "    return count_same/len(label1)\n",
        "\n",
        "def kohens_cappa(annotator1_df, annotator2_df):\n",
        "    # TODO: Implement this function\n",
        "    df1 = pd.read_csv(annotator1_df, sep='\\t')\n",
        "    df2 = pd.read_csv(annotator2_df, sep='\\t')\n",
        "    label1 =  df1['label']\n",
        "    label2 =  df2['label']\n",
        "    return cohen_kappa_score(label1,label2)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: load your data correctly\n",
        "annotator1_df = \"annotator1.tsv\"\n",
        "annotator2_df = \"annotator2.tsv\"\n",
        "\n",
        "print(\"--- Raw agreement between annotator1 and annotator2 ---\")\n",
        "print(raw_agreement(annotator1_df, annotator2_df))\n",
        "\n",
        "print(\"--- Cohen's kappa score between annotator1 and annotator2 ---\")\n",
        "print(kohens_cappa(annotator1_df, annotator2_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd7Xq4SKzF5U"
      },
      "source": [
        "TODO : Write the values obtained above in this cell.\n",
        "\n",
        "### *RAW AGREEMENT*:  0.7577639751552795\n",
        "\n",
        "### *COHEN'S KAPPA*:  0.7395578414699905"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23zfO_ALKeB"
      },
      "source": [
        "# Checkpoint 2: Model Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N25dvF4jvYoy"
      },
      "source": [
        "Now we'll move onto fine-tuning  pretrained language models specifically on your dataset. This part of the homework is meant to be an introduction to the HuggingFace library, and it contains code that will potentially be useful for your final projects. Since we're dealing with large models, the first step is to change to a GPU runtime.\n",
        "\n",
        "## Adding a hardware accelerator\n",
        "\n",
        "Please go to the menu and add a GPU as follows:\n",
        "\n",
        "`Edit > Notebook Settings > Hardware accelerator > (GPU)`\n",
        "\n",
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edOh9ooiIW1B",
        "outputId": "b839442d-8285-46fe-b4de-06643842a5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-hTdslAblxP8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvH7xx9LnMC"
      },
      "source": [
        "## Installing Hugging Face's Transformers library\n",
        "We will use Hugging Face's Transformers (https://github.com/huggingface/transformers), an open-source library that provides general-purpose architectures for natural language understanding and generation with a collection of various pretrained models made by the NLP community. This library will allow us to easily use pretrained models like `BERT` and perform experiments on top of them. We can use these models to solve downstream target tasks, such as text classification, question answering, and sequence labeling.\n",
        "\n",
        "Run the following cell to install Hugging Face's Transformers library and download a sample data file called seed.tsv that contains 250 sentences in English, annotated with their frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtqS2e5fxpqa",
        "outputId": "15507657-8165-470e-f231-44cd6192d90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8XIL7wPovVX"
      },
      "source": [
        "The cell below imports some helper functions we wrote to demonstrate the task on\n",
        "the sample seed dataset.\n",
        "\n",
        "#### *IMPORTANT NOTE*:\n",
        "\n",
        "The tokenize_and_format function in helpers.py uses bert-base-uncased as the\n",
        "model for the tokenizer. If you are using a different model for training in this\n",
        "notebook or for running predictions in a different notebook or python file, you\n",
        "need to change the model name as well in the tokenizer, otherwise you will get\n",
        "arbitrarily incorrect results down the line.\n",
        "\n",
        "If you update the model name for the tokenizer, you would need to reload the\n",
        "file which can be done simply by re-running the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YHpr9qh0nV8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Taseb33Sovg0"
      },
      "outputs": [],
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKc0xYh-MAbc"
      },
      "source": [
        "# Data Prep and Model Specifications\n",
        "\n",
        "Upload your data using the file explorer to the left. We have provided a\n",
        "function below to tokenize and format your data as BERT requires. Make sure that\n",
        "your tsv file, titled final_data.tsv, has one column \"sentence\" and another\n",
        "column \"label_ID\" containing integers/float. (basically the same format as\n",
        "seed.tsv should be maintained for the sentence and label columns)\n",
        "\n",
        "If you run the cell below without modifications, it will run on the seed.tsv\n",
        "example data we have provided. It imports some helper functions we wrote to\n",
        "demonstrate the task on the sample dataset. You should first run all of the\n",
        "following cells with seed.tsv just to see how everything works. Then, once you\n",
        "understand the whole preprocessing / fine-tuning process, change the tsv in the\n",
        "below cell to your final_data.tsv file, add any extra preprocessing code you\n",
        "wish, and then run the cells again on your own data.\n",
        "\n",
        "\n",
        "#### Important Note :\n",
        "\n",
        "The code below expects the data to be in a tsv file  with the columns as \"sentence\"\n",
        "and \"label_ID\" (other columns are not that relevant here). But this is different\n",
        "from the instructions in the report where you are expected to create data with\n",
        "\"text\" and \"label\" columns for all of the annotation steps.\n",
        "\n",
        "Modify the code below to suitably handle this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGhkeLQlNNr8",
        "outputId": "723e9e84-cd0c-4e2a-c821-0fd6bd4434ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Legal recognition of same-sex marriage contributes to the health and safety of LGBTQ couples by providing access to crucial healthcare benefits, including insurance coverage for spouses.\n",
            "Token IDs: tensor([  101,  3423,  5038,  1997,  2168,  1011,  3348,  3510, 16605,  2000,\n",
            "         1996,  2740,  1998,  3808,  1997, 12010,  4160,  6062,  2011,  4346,\n",
            "         3229,  2000, 10232,  9871,  6666,  1010,  2164,  5427,  6325,  2005,\n",
            "        18591,  2015,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "#df = pd.read_csv('final_data.tsv') # TODO : Uncomment this line to use the full dataset\n",
        "df = pd.read_csv('dtangira_final_data.tsv',sep='\\t')\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "texts = df.text.values # this assumes that the column containing the text is called \"sentence\"\n",
        "labels = df.label.values # this assumes that the column containing the labels is called \"label_ID\"\n",
        "temp_labels = labels # Store the Labels before converting into\n",
        "\n",
        "### tokenize_and_format() is a helper function provided in helpers.py ###\n",
        "### Male sure you use the correct model name for your tokenizer! ###\n",
        "input_ids, attention_masks = tokenize_and_format(texts)\n",
        "\n",
        "label_list = []\n",
        "for l in labels:\n",
        "  label_array = np.zeros(len(set(labels)))\n",
        "  label_array[int(l)-1] = 1\n",
        "  label_list.append(label_array)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(np.array(label_list))\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3D-CzQEUXYz"
      },
      "source": [
        "## Create train/test/validation splits\n",
        "\n",
        "Here we split your dataset into 3 parts: a training set, a validation set, and a testing set. Each item in your dataset will be a 3-tuple containing an input_id tensor, an attention_mask tensor, and a label tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kGgeZ3M0UWs0"
      },
      "outputs": [],
      "source": [
        "seed_everything()\n",
        "\n",
        "total = len(df)\n",
        "\n",
        "num_train = int(total * .8)\n",
        "num_val = int(total * .1)\n",
        "num_test = total - num_train - num_val\n",
        "\n",
        "# make lists of 3-tuples (already shuffled the dataframe in cell above)\n",
        "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
        "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
        "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
        "\n",
        "train_text = [texts[i] for i in range(num_train)]\n",
        "train_label = [temp_labels[i] for i in range(num_train)]\n",
        "\n",
        "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
        "test_text = [texts[i] for i in range(num_val + num_train, total)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep and Model Specifications for New Multilingual Data *****"
      ],
      "metadata": {
        "id": "pdQ484Cn7eXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "#df = pd.read_csv('final_data.tsv') # TODO : Uncomment this line to use the full dataset\n",
        "df_new = pd.read_csv('dtangira_augmented_data.tsv',sep='\\t')\n",
        "\n",
        "df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
        "texts = df_new.text.values # this assumes that the column containing the text is called \"sentence\"\n",
        "labels = df_new.label.values # this assumes that the column containing the labels is called \"label_ID\"\n",
        "#temp_labels = labels # Store the Labels before converting into\n",
        "print(len(labels))\n",
        "\n",
        "### tokenize_and_format() is a helper function provided in helpers.py ###\n",
        "### Male sure you use the correct model name for your tokenizer! ###\n",
        "input_ids, attention_masks = tokenize_and_format(texts)\n",
        "\n",
        "label_list = []\n",
        "for l in labels:\n",
        "  label_array = np.zeros(len(set(labels)))\n",
        "  label_array[int(l)-1] = 1\n",
        "  label_list.append(label_array)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(np.array(label_list))\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InTG_3P97dzM",
        "outputId": "88796e41-02c0-4e67-cda2-92dedd9d410a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4016\n",
            "Original:  धार्मिकता की भावना से प्रेरित होकर, धार्मिक समुदाय आप्रवासियों के अधिकारों का समर्थन करने और मानवीय गरिमा को बनाए रखने वाले व्यापक आप्रवासन सुधार की वकालत करने के लिए संगठित हो सकते हैं।\n",
            "Token IDs: tensor([  101,  1326, 29876, 29869, 29867, 29877, 29851, 29859, 29876,  1315,\n",
            "        29878,  1330, 29876, 29871, 29863, 29876,  1338,  1328, 29869, 29869,\n",
            "        29877, 29859,  1339, 29879, 29851, 29869,  1010,  1326, 29876, 29869,\n",
            "        29867, 29877, 29851,  1338, 29867, 29861, 29876, 29868,  1312, 29864,\n",
            "        29869, 29871, 29876, 29874, 29877, 29868, 29879,  1315,  1311, 29862,\n",
            "        29877, 29851, 29876, 29869, 29879,  1315, 29876,  1338, 29867, 29869,\n",
            "        29860, 29863,  1315,   102])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create train splits for Multilingual Data ****"
      ],
      "metadata": {
        "id": "wWLeZdoy9JAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything()\n",
        "num_train = len(df_new)\n",
        "\n",
        "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
        "train_text = [texts[i] for i in range(num_train)]"
      ],
      "metadata": {
        "id": "AqmjQEhv9Im7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_set))\n",
        "print(len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEj8jlA8Nrmf",
        "outputId": "723cdcba-1cd3-478c-cc3a-61c217906d58"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4016\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCr006iTkqwM"
      },
      "source": [
        "Here we choose the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. Because the task requires us to label sentences, we wil be using BertForSequenceClassification below. You may see a warning that states that `some weights of the model checkpoint at [model name] were not used when initializing. . .` This warning is expected and means that you should fine-tune your pre-trained model before using it on your downstream task. See [here](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854) for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "lPo640_ZlEPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dab2054-2210-4830-ae38-28b00f2ccaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer English BERT model, with an uncased vocab.\n",
        "    num_labels = 15, # The number of output labels.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose a MODEL for BERT ****"
      ],
      "metadata": {
        "id": "Fx7RvqVA-cNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use model of the top 104 languages with the largest Wikipedia\n",
        "    num_labels = 15, # The number of output labels.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502,
          "referenced_widgets": [
            "68491ab29325498c858d12cd9c454f63",
            "73f9bdfbd2494f7ba44b37b3638732c4",
            "14c1f49315c74362873fb746fbf9d311",
            "ac3743e63e084c4eb1870f84e32ea43e",
            "dee76b36a7d34381bbd14ac26ad085bc",
            "95b58766e03c4e64a24c329647496741",
            "25e904109c47409c8d9cf2bd8dcdaf60",
            "918b556e10bb488bba09f8364db8ce99",
            "825d844d6c964883b45f5bd7bf37d005",
            "4a8eaef8aa2f4a76abede1dd09f1cd36",
            "c92bf458d08548b197e600066c501abd",
            "323f3d8653644875810bfbc091877aaf",
            "d510c672c7714032836b7d23d5534833",
            "87ff001dfa9e49ceaa84bb438fd5b680",
            "edc98e37718e47a498c1634f85307c6b",
            "08375046a5af450a824e24f7505b5b85",
            "31d0e6bec3854b3a9e95e6577d6fad44",
            "bc6907a923b9495a8e81be41252a343e",
            "692bd59453e649898eb705ba9bc3eb55",
            "6103fffb36fa42459004fc0aa5cfb3e1",
            "3815ad40834542349cfd35374fc072ea",
            "7947cbde529d4677aa01004d5874e09d"
          ]
        },
        "id": "1hdPXxyb-nf1",
        "outputId": "e76bd345-68fd-4f1b-a560-d90d7ebf2448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68491ab29325498c858d12cd9c454f63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "323f3d8653644875810bfbc091877aaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 87.06 MiB is free. Process 4783 has 14.66 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 426.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-44b159e1a8c5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Tell pytorch to run this model on the GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2526\u001b[0m             )\n\u001b[1;32m   2527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2528\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \"\"\"\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \"\"\"\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 87.06 MiB is free. Process 4783 has 14.66 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 426.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3lLdoW_le3M"
      },
      "source": [
        "# TODO: ACTION REQUIRED #\n",
        "\n",
        "Define your fine-tuning hyperparameters in the cell below (we have randomly picked some values to start with). We want you to experiment with different configurations to find the one that works best (i.e., highest accuracy) on your validation set. Feel free to also change pretrained models to others available in the HuggingFace library (you'll have to modify the cell above to do this). You might find papers on BERT fine-tuning stability (e.g., [Mosbach et al., ICLR 2021](https://openreview.net/pdf?id=nzpLWnVAyah)) to be of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd2JdC6IletV"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "# you can change lr and eps values in the AdamW call if you like\n",
        "optimizer = AdamW(model.parameters(),lr=5e-5,eps=1e-6) #with default values of learning rate and epsilon value\n",
        "epochs =  16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose hyper parameters for Multilingual Model ****"
      ],
      "metadata": {
        "id": "PgGCJAEt_Fl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "# you can change lr and eps values in the AdamW call if you like\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-5) #with default values of learning rate and epsilon value\n",
        "epochs =  50\n",
        "\n",
        "batch_size = 64\n",
        "# you can change lr and eps values in the AdamW call if you like\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-5) #with default values of learning rate and epsilon value\n",
        "epochs =  60\n"
      ],
      "metadata": {
        "id": "jqvTUJI0_mRZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd4fwn_el1ge"
      },
      "source": [
        "# Fine-tune your model\n",
        "Here we provide code for fine-tuning your model, monitoring the loss, and checking your validation accuracy. Rerun both of the below cells when you change your hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "O_Mzr-kd5RaY"
      },
      "outputs": [],
      "source": [
        "# function to get validation accuracy\n",
        "def get_validation_performance(val_set):\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    num_batches = int(len(val_set)/batch_size) + 1\n",
        "\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      end_index = min(batch_size * (i+1), len(val_set))\n",
        "\n",
        "      batch = val_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():\n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # Note: this line of code might need to change depending on the model\n",
        "        # the current line will work for bert-base-uncased\n",
        "        # please refer to huggingface documentation for other models\n",
        "        outputs = model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = (logits).detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        # Calculate the number of correctly labeled examples in batch\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = np.argmax(label_ids, axis=1).flatten()\n",
        "\n",
        "        num_correct = np.sum(pred_flat == labels_flat)\n",
        "        total_correct += num_correct\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"Num of correct predictions =\", total_correct)\n",
        "    avg_val_accuracy = total_correct / len(val_set)\n",
        "    return avg_val_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTf_ipbjWNoV",
        "outputId": "0acf796a-bb36-47a4-f91c-1927b7acdaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 60 ========\n",
            "Training...\n",
            "Total loss: 27.36504389930893\n",
            "Num of correct predictions = 0\n",
            "Validation accuracy: 0.0\n",
            "\n",
            "======== Epoch 2 / 60 ========\n",
            "Training...\n",
            "Total loss: 16.277714449940763\n",
            "Num of correct predictions = 2\n",
            "Validation accuracy: 0.06451612903225806\n",
            "\n",
            "======== Epoch 3 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.494554685697794\n",
            "Num of correct predictions = 2\n",
            "Validation accuracy: 0.06451612903225806\n",
            "\n",
            "======== Epoch 4 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.3880233369315\n",
            "Num of correct predictions = 2\n",
            "Validation accuracy: 0.06451612903225806\n",
            "\n",
            "======== Epoch 5 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.381063232956354\n",
            "Num of correct predictions = 2\n",
            "Validation accuracy: 0.06451612903225806\n",
            "\n",
            "======== Epoch 6 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.372192070238656\n",
            "Num of correct predictions = 5\n",
            "Validation accuracy: 0.16129032258064516\n",
            "\n",
            "======== Epoch 7 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.33553467752386\n",
            "Num of correct predictions = 5\n",
            "Validation accuracy: 0.16129032258064516\n",
            "\n",
            "======== Epoch 8 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.315193026742454\n",
            "Num of correct predictions = 4\n",
            "Validation accuracy: 0.12903225806451613\n",
            "\n",
            "======== Epoch 9 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.262441492746838\n",
            "Num of correct predictions = 6\n",
            "Validation accuracy: 0.1935483870967742\n",
            "\n",
            "======== Epoch 10 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.217322266207901\n",
            "Num of correct predictions = 6\n",
            "Validation accuracy: 0.1935483870967742\n",
            "\n",
            "======== Epoch 11 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.159605649500413\n",
            "Num of correct predictions = 7\n",
            "Validation accuracy: 0.22580645161290322\n",
            "\n",
            "======== Epoch 12 / 60 ========\n",
            "Training...\n",
            "Total loss: 15.084572634868609\n",
            "Num of correct predictions = 6\n",
            "Validation accuracy: 0.1935483870967742\n",
            "\n",
            "======== Epoch 13 / 60 ========\n",
            "Training...\n",
            "Total loss: 14.991681815804135\n",
            "Num of correct predictions = 10\n",
            "Validation accuracy: 0.3225806451612903\n",
            "\n",
            "======== Epoch 14 / 60 ========\n",
            "Training...\n",
            "Total loss: 14.86160813132301\n",
            "Num of correct predictions = 11\n",
            "Validation accuracy: 0.3548387096774194\n",
            "\n",
            "======== Epoch 15 / 60 ========\n",
            "Training...\n",
            "Total loss: 14.71181559761446\n",
            "Num of correct predictions = 14\n",
            "Validation accuracy: 0.45161290322580644\n",
            "\n",
            "======== Epoch 16 / 60 ========\n",
            "Training...\n",
            "Total loss: 14.486696386305425\n",
            "Num of correct predictions = 14\n",
            "Validation accuracy: 0.45161290322580644\n",
            "\n",
            "======== Epoch 17 / 60 ========\n",
            "Training...\n",
            "Total loss: 14.191436319799042\n",
            "Num of correct predictions = 17\n",
            "Validation accuracy: 0.5483870967741935\n",
            "\n",
            "======== Epoch 18 / 60 ========\n",
            "Training...\n",
            "Total loss: 13.905039668561702\n",
            "Num of correct predictions = 16\n",
            "Validation accuracy: 0.5161290322580645\n",
            "\n",
            "======== Epoch 19 / 60 ========\n",
            "Training...\n",
            "Total loss: 13.44066218530748\n",
            "Num of correct predictions = 18\n",
            "Validation accuracy: 0.5806451612903226\n",
            "\n",
            "======== Epoch 20 / 60 ========\n",
            "Training...\n",
            "Total loss: 12.880779840148092\n",
            "Num of correct predictions = 19\n",
            "Validation accuracy: 0.6129032258064516\n",
            "\n",
            "======== Epoch 21 / 60 ========\n",
            "Training...\n",
            "Total loss: 12.461390789642453\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 22 / 60 ========\n",
            "Training...\n",
            "Total loss: 11.803329518816708\n",
            "Num of correct predictions = 19\n",
            "Validation accuracy: 0.6129032258064516\n",
            "\n",
            "======== Epoch 23 / 60 ========\n",
            "Training...\n",
            "Total loss: 11.123741417558875\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 24 / 60 ========\n",
            "Training...\n",
            "Total loss: 10.455656295576047\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 25 / 60 ========\n",
            "Training...\n",
            "Total loss: 9.958374872605058\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 26 / 60 ========\n",
            "Training...\n",
            "Total loss: 9.309794351255798\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 27 / 60 ========\n",
            "Training...\n",
            "Total loss: 8.727427762681408\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 28 / 60 ========\n",
            "Training...\n",
            "Total loss: 7.95508154729889\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 29 / 60 ========\n",
            "Training...\n",
            "Total loss: 7.130850864572494\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 30 / 60 ========\n",
            "Training...\n",
            "Total loss: 6.534006193345299\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 31 / 60 ========\n",
            "Training...\n",
            "Total loss: 6.234256907512221\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 32 / 60 ========\n",
            "Training...\n",
            "Total loss: 5.715620636523659\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 33 / 60 ========\n",
            "Training...\n",
            "Total loss: 5.149764180988577\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 34 / 60 ========\n",
            "Training...\n",
            "Total loss: 4.800408772104775\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 35 / 60 ========\n",
            "Training...\n",
            "Total loss: 4.496627259662858\n",
            "Num of correct predictions = 22\n",
            "Validation accuracy: 0.7096774193548387\n",
            "\n",
            "======== Epoch 36 / 60 ========\n",
            "Training...\n",
            "Total loss: 4.216466099369428\n",
            "Num of correct predictions = 23\n",
            "Validation accuracy: 0.7419354838709677\n",
            "\n",
            "======== Epoch 37 / 60 ========\n",
            "Training...\n",
            "Total loss: 4.028886687445872\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 38 / 60 ========\n",
            "Training...\n",
            "Total loss: 3.5910401030224723\n",
            "Num of correct predictions = 23\n",
            "Validation accuracy: 0.7419354838709677\n",
            "\n",
            "======== Epoch 39 / 60 ========\n",
            "Training...\n",
            "Total loss: 3.4047879894654516\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 40 / 60 ========\n",
            "Training...\n",
            "Total loss: 3.1829861897515803\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 41 / 60 ========\n",
            "Training...\n",
            "Total loss: 3.138918176702484\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 42 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.9391665002171683\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 43 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.790182632952827\n",
            "Num of correct predictions = 22\n",
            "Validation accuracy: 0.7096774193548387\n",
            "\n",
            "======== Epoch 44 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.672585310075717\n",
            "Num of correct predictions = 22\n",
            "Validation accuracy: 0.7096774193548387\n",
            "\n",
            "======== Epoch 45 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.4793825434303676\n",
            "Num of correct predictions = 22\n",
            "Validation accuracy: 0.7096774193548387\n",
            "\n",
            "======== Epoch 46 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.417570664320243\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 47 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.311622017394813\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 48 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.2807453807387605\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 49 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.1598615763353646\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 50 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.131712994040542\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 51 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.1099765632723684\n",
            "Num of correct predictions = 22\n",
            "Validation accuracy: 0.7096774193548387\n",
            "\n",
            "======== Epoch 52 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.0737284748886644\n",
            "Num of correct predictions = 19\n",
            "Validation accuracy: 0.6129032258064516\n",
            "\n",
            "======== Epoch 53 / 60 ========\n",
            "Training...\n",
            "Total loss: 2.0132396861020614\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 54 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.9987508404501368\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 55 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.9527527317267306\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 56 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.7902937501408707\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 57 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.799211283462753\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 58 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.7406410552618072\n",
            "Num of correct predictions = 21\n",
            "Validation accuracy: 0.6774193548387096\n",
            "\n",
            "======== Epoch 59 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.6543442429938853\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "======== Epoch 60 / 60 ========\n",
            "Training...\n",
            "Total loss: 1.6323832379184149\n",
            "Num of correct predictions = 20\n",
            "Validation accuracy: 0.6451612903225806\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "seed_everything()\n",
        "\n",
        "# training loop\n",
        "\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    num_batches = int(len(train_set)/batch_size) + 1\n",
        "\n",
        "    for i in range(num_batches):\n",
        "      end_index = min(batch_size * (i+1), len(train_set))\n",
        "\n",
        "      batch = train_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform a forward pass (evaluate the model on this training batch).\n",
        "      # this line of code might need to change depending on the model\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "      loss = outputs.loss\n",
        "      logits = outputs.logits\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate the gradients.\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters and take a step using the computed gradient.\n",
        "      optimizer.step()\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set. Implement this function in the cell above.\n",
        "    print(f\"Total loss: {total_train_loss}\")\n",
        "    val_acc = get_validation_performance(val_set)\n",
        "    print(f\"Validation accuracy: {val_acc}\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx_HXpmvzaAf",
        "outputId": "b3be0f3b-9e1f-4078-a241-5dca2ec1ca47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: SAVE YOUR MODEL HERE... (Refer PyTorch documentation for how to save models)\n",
        "torch.save(model.state_dict(),\"/content/drive/MyDrive/Colab_Notebooks/Best.pt\")"
      ],
      "metadata": {
        "id": "eNOnY8Nx3FV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE NEW MULTILINGUAL MODEL...\n",
        "torch.save(model.state_dict(),\"/content/drive/MyDrive/Colab_Notebooks/Multi_Best.pt\")"
      ],
      "metadata": {
        "id": "jv8VBgQVAYZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DpRJE5mHkO"
      },
      "source": [
        "# Evaluate your model on the test set\n",
        "After you're satisfied with your hyperparameters (i.e., you're unable to achieve higher validation accuracy by modifying them further), it's time to evaluate your model on the test set! Run the below cell to compute test set accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msvZ78ii3cZZ",
        "outputId": "c9ac438d-c868-4a23-c7b5-7b7d3115d7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of correct predictions = 19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.59375"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "seed_everything()\n",
        "\n",
        "# If your notebook disconnects during training, then here, first load the best\n",
        "# model you saved (refer PyTorch docs), then check validation performance\n",
        "\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Best.pt\"))\n",
        "get_validation_performance(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate your Multi-Lingual model on the test set ****"
      ],
      "metadata": {
        "id": "cjeS6eqlDPZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything()\n",
        "\n",
        "# If your notebook disconnects during training, then here, first load the best\n",
        "# model you saved (refer PyTorch docs), then check validation performance\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Multi_Best.pt\"))\n",
        "get_validation_performance(test_set)"
      ],
      "metadata": {
        "id": "C_XF7whVDMEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b56284-ce24-4aa1-b510-2d0808224f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of correct predictions = 25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.78125"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBbdMwt79fIs"
      },
      "source": [
        "## Question 8 (10 points):\n",
        "Finally, perform an *error analysis* on your model. This is good practice for your final project. Write some code in the below code cell to print out the text of up to five test set examples that your model gets **wrong**. If your model gets more than five test examples wrong, randomly choose five of them to analyze. If your model gets fewer than five examples wrong, please design five test examples that fool your model (i.e., *adversarial examples*). Then, in the following text cell, perform a qualitative analysis of these examples. See if you can figure out any reasons for errors that you observe, or if you have any informed guesses (e.g., common linguistic properties of these particular examples). Does this analysis suggest any possible future steps to improve your classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X72mumhI9WdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df985d61-16c2-41c5-bded-35a49fbef38e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advocates for immigration equality argue for policies that prioritize human rights, dignity, and inclusivity, recognizing the contributions and humanity of all individuals regardless of immigration status.\n",
            "Predicted Label 10\n",
            "Correct Label 3\n",
            "In recent decades, immigration patterns have led to more cross-cultural experiences on both sides of the border.\n",
            "Predicted Label 9\n",
            "Correct Label 13\n",
            "The costs of immigration can include expenditures on public services such as education, healthcare, and social welfare programs for immigrants and their families.\n",
            "Predicted Label 0\n",
            "Correct Label 9\n",
            "Limited access to legal representation for migrants, particularly those facing deportation proceedings, undermines due process and fairness within the immigration system.\n",
            "Predicted Label 3\n",
            "Correct Label 1\n",
            "Marriage equality grants same-sex couples legal protections against domestic violence and abuse, previously more difficult to address.\n",
            "Predicted Label 5\n",
            "Correct Label 6\n"
          ]
        }
      ],
      "source": [
        "## YOUR ERROR ANALYSIS CODE HERE\n",
        "\n",
        "## print out up to 5 test set examples (or adversarial examples) that your model gets wrong\n",
        "seed_everything()\n",
        "torch.cuda.empty_cache()\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Best.pt\"))\n",
        "#get_validation_performance(test_set)\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "\n",
        "num_batches = int(len(test_set)/batch_size) + 1\n",
        "\n",
        "total_correct = 0\n",
        "error_pred = []\n",
        "for i in range(num_batches):\n",
        "  end_index = min(batch_size * (i+1), len(test_set))\n",
        "  batch = test_set[i*batch_size:end_index]\n",
        "  if len(batch) == 0: continue\n",
        "  input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "  input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "  label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "  # Move tensors to the GPU\n",
        "  b_input_ids = input_id_tensors.to(device)\n",
        "  b_input_mask = input_mask_tensors.to(device)\n",
        "  b_labels = label_tensors.to(device)\n",
        "\n",
        "  # Tell pytorch not to bother with constructing the compute graph during\n",
        "  # the forward pass, since this is only needed for backprop (training).\n",
        "  with torch.no_grad():\n",
        "\n",
        "\n",
        "    # Forward pass, calculate logit predictions.\n",
        "    # Note: this line of code might need to change depending on the model\n",
        "    # the current line will work for bert-base-uncased\n",
        "    # please refer to huggingface documentation for other models\n",
        "    outputs = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Accumulate the validation loss.\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = (logits).detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate the number of correctly labeled examples in batch\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "    labels_flat = np.argmax(label_ids, axis=1).flatten()\n",
        "    for ind in range(len(batch)):\n",
        "      if pred_flat[ind] != labels_flat[ind]:\n",
        "        error_pred.append((test_text[ind], pred_flat[ind], labels_flat[ind]))\n",
        "\n",
        "    num_correct = np.sum(pred_flat == labels_flat)\n",
        "    total_correct += num_correct\n",
        "\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  print(error_pred[i][0])\n",
        "  print(\"Predicted Label\",error_pred[i][1])\n",
        "  print(\"Correct Label\",error_pred[i][2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==3.1.0a0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwL60WwAWbid",
        "outputId": "d418b8c6-244d-42c7-960c-7776616ccbcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2024.3.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16353 sha256=1d31206c20b9f7f039ab916672225ca0790da81604a36038171c4e0a3609737f\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.3.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation for seed.tsv and training data"
      ],
      "metadata": {
        "id": "oFZmK4MO-R5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install googletrans first using one of the methods below:\n",
        "# pip install googletrans==4.0.0-rc1\n",
        "# pip install googletrans==3.1.0a0\n",
        "# pip install google_trans_new\n",
        "seed_everything()\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "import pandas as pd\n",
        "import time\n",
        "# see available languages with the below\n",
        "print(googletrans.LANGUAGES)\n",
        "\n",
        "# Init\n",
        "translator = Translator()\n",
        "\n",
        "# Call to translate and get output\n",
        "output = translator.translate(\"I love natural language processing\", src='en', dest='fr')\n",
        "#print(output.text)\n",
        "df = pd.read_csv('seed_New - seed.tsv', sep='\\t')\n",
        "\n",
        "#df = pd.read_csv('dtangira_final_data.tsv', sep='\\t')\n",
        "\n",
        "print(len(list(df['label_ID']) ))\n",
        "print (len(train_text))\n",
        "label =  train_label + list(df['label_ID']) # Only use training set label and Seed.tsv data\n",
        "\n",
        "#print(train_label)\n",
        "text = train_text + list(df['sentence']) # Only use training set data and seed.tsv data\n",
        "#language = df['language']\n",
        "print (len(label))\n",
        "print (len(text))\n",
        "all_lang = ['en','zh-cn','hi','te','bn','el','ru','tr']\n",
        "\n",
        "columns = ['text', 'language','label','source']\n",
        "new_text =[]\n",
        "new_language = []\n",
        "new_label = []\n",
        "new_source = []\n",
        "new_annotator = []\n",
        "length = len(text)\n",
        "for i in all_lang:\n",
        "    print(i)\n",
        "    if(i == 'en'):\n",
        "        for line in range(length):\n",
        "            #time.sleep(1)\n",
        "            temp = translator.translate(text[line], src='en',dest='te')\n",
        "            new_text.append((translator.translate(temp.text, src='te',dest='en')).text)\n",
        "            new_language.append(i)\n",
        "            new_source.append('en'+'->'+'en')\n",
        "            new_label.append(label[line])\n",
        "\n",
        "    else:\n",
        "        for line in range(length):\n",
        "\n",
        "            #temp = translator.translate(text[line],src='eng',dest='hin')\n",
        "\n",
        "            new_text.append((translator.translate(text[line],src='en',dest=i)).text)\n",
        "            new_language.append(i)\n",
        "            new_source.append('en' + '->' + i)\n",
        "            new_label.append(label[line])\n",
        "data = {}\n",
        "data['text'] = new_text\n",
        "data['language'] = new_language\n",
        "data['label'] = new_label\n",
        "data['source'] = new_source\n",
        "df = pd.DataFrame(data=data)\n",
        "output_file = 'dtangira_augmented_data.tsv'\n",
        "\n",
        "df.to_csv(output_file, sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly8E3ViECr0t",
        "outputId": "087654e4-1978-49b3-cd34-fae81452cc28"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n",
            "250\n",
            "252\n",
            "502\n",
            "502\n",
            "en\n",
            "zh-cn\n",
            "hi\n",
            "te\n",
            "bn\n",
            "el\n",
            "ru\n",
            "tr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions from JSON ****"
      ],
      "metadata": {
        "id": "Vb1Kwv-hyHXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "seed_everything()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Multi_Best.pt\"))\n",
        "#model.eval()\n",
        "id_mapping = {0: 'None', 1: 'Economic', 2: 'Capacity and Resources', 3: 'Morality', 4: 'Fairness and Equality', 5: 'Legality, Constitutionality, Jurisdiction', 6: 'Policy Prescription and Evaluation', 7: 'Crime and Punishment', 8: 'Security and Defense', 9: 'Health and Safety', 10: 'Quality of Life', 11: 'Cultural Identity', 12: 'Public Sentiment', 13: 'Political', 14: 'External Regulation and Reputation', 15: 'Other'}\n",
        "\n",
        "\n",
        "# Tracking variables\n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "\n",
        "with open(\"test_set.json\", \"r\") as f:\n",
        "    test_set = json.load(f)\n",
        "\n",
        "\n",
        "num_batches = int(len(test_set)/batch_size) + 1\n",
        "\n",
        "total_correct = 0\n",
        "\n",
        "for i in range(num_batches):\n",
        "  end_index = min(batch_size * (i+1), len(test_set))\n",
        "  batch = test_set[i*batch_size:end_index]\n",
        "  if len(batch) == 0: continue\n",
        "  texts = []\n",
        "  for data in batch:\n",
        "    texts.append(data['sentence'])\n",
        "  input_ids, attention_masks = tokenize_and_format(texts)\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "  # Move tensors to the GPU\n",
        "  b_input_ids = input_ids.to(device)\n",
        "  b_input_mask = attention_masks.to(device)\n",
        "  #b_labels = label_tensors.to(device)\n",
        "\n",
        "  # Tell pytorch not to bother with constructing the compute graph during\n",
        "  # the forward pass, since this is only needed for backprop (training).\n",
        "  with torch.no_grad():\n",
        "\n",
        "\n",
        "    # Forward pass, calculate logit predictions.\n",
        "    # Note: this line of code might need to change depending on the model\n",
        "    # the current line will work for bert-base-uncased\n",
        "    # please refer to huggingface documentation for other models\n",
        "    outputs = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask)\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "    error_pred = []\n",
        "    # Accumulate the validation loss.\n",
        "    #total_eval_loss += loss.item()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = (logits).detach().cpu().numpy()\n",
        "    # Calculate the number of correctly labeled examples in batch\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "    for ind in range(len(pred_flat)):\n",
        "      test_set[ind + (i*batch_size)]['label_ID'] = str(pred_flat[ind])\n",
        "      test_set[ind + (i*batch_size)]['predicted_label'] = id_mapping[pred_flat[ind]]\n",
        "\n",
        "\n",
        "with open(\"dtangira.json\", \"w\") as f:\n",
        "    json.dump(test_set, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dK5HF_aMyGtd"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XyBdAup-e6Z"
      },
      "source": [
        "### *DESCRIBE YOUR QUALITATIVE ANALYSIS OF THE ABOVE EXAMPLES IN YOUR REPORT*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SgNZTjrhcHa0"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68491ab29325498c858d12cd9c454f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73f9bdfbd2494f7ba44b37b3638732c4",
              "IPY_MODEL_14c1f49315c74362873fb746fbf9d311",
              "IPY_MODEL_ac3743e63e084c4eb1870f84e32ea43e"
            ],
            "layout": "IPY_MODEL_dee76b36a7d34381bbd14ac26ad085bc"
          }
        },
        "73f9bdfbd2494f7ba44b37b3638732c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95b58766e03c4e64a24c329647496741",
            "placeholder": "​",
            "style": "IPY_MODEL_25e904109c47409c8d9cf2bd8dcdaf60",
            "value": "config.json: 100%"
          }
        },
        "14c1f49315c74362873fb746fbf9d311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_918b556e10bb488bba09f8364db8ce99",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_825d844d6c964883b45f5bd7bf37d005",
            "value": 625
          }
        },
        "ac3743e63e084c4eb1870f84e32ea43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a8eaef8aa2f4a76abede1dd09f1cd36",
            "placeholder": "​",
            "style": "IPY_MODEL_c92bf458d08548b197e600066c501abd",
            "value": " 625/625 [00:00&lt;00:00, 42.2kB/s]"
          }
        },
        "dee76b36a7d34381bbd14ac26ad085bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b58766e03c4e64a24c329647496741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e904109c47409c8d9cf2bd8dcdaf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "918b556e10bb488bba09f8364db8ce99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825d844d6c964883b45f5bd7bf37d005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a8eaef8aa2f4a76abede1dd09f1cd36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92bf458d08548b197e600066c501abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "323f3d8653644875810bfbc091877aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d510c672c7714032836b7d23d5534833",
              "IPY_MODEL_87ff001dfa9e49ceaa84bb438fd5b680",
              "IPY_MODEL_edc98e37718e47a498c1634f85307c6b"
            ],
            "layout": "IPY_MODEL_08375046a5af450a824e24f7505b5b85"
          }
        },
        "d510c672c7714032836b7d23d5534833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d0e6bec3854b3a9e95e6577d6fad44",
            "placeholder": "​",
            "style": "IPY_MODEL_bc6907a923b9495a8e81be41252a343e",
            "value": "model.safetensors: 100%"
          }
        },
        "87ff001dfa9e49ceaa84bb438fd5b680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_692bd59453e649898eb705ba9bc3eb55",
            "max": 714290682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6103fffb36fa42459004fc0aa5cfb3e1",
            "value": 714290682
          }
        },
        "edc98e37718e47a498c1634f85307c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3815ad40834542349cfd35374fc072ea",
            "placeholder": "​",
            "style": "IPY_MODEL_7947cbde529d4677aa01004d5874e09d",
            "value": " 714M/714M [00:08&lt;00:00, 111MB/s]"
          }
        },
        "08375046a5af450a824e24f7505b5b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d0e6bec3854b3a9e95e6577d6fad44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc6907a923b9495a8e81be41252a343e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "692bd59453e649898eb705ba9bc3eb55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6103fffb36fa42459004fc0aa5cfb3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3815ad40834542349cfd35374fc072ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7947cbde529d4677aa01004d5874e09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}